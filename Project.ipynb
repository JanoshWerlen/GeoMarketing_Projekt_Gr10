{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555aa4e1",
   "metadata": {},
   "source": [
    "# Database Connection Setup\n",
    "This cell sets up the connection to the local PostgreSQL/PostGIS database using SQLAlchemy.\n",
    "\n",
    "**Credentials are now loaded from the `.env` file using `python-dotenv`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://postgres:1794682350.@localhost:5432/geomarketing\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "user = os.getenv('LOCAL_DB_USER', 'postgres')\n",
    "password = os.getenv('LOCAL_DB_PASSWORD')\n",
    "host = os.getenv('LOCAL_DB_HOST', 'localhost')\n",
    "port = os.getenv('LOCAL_DB_PORT', '5432')\n",
    "db = os.getenv('LOCAL_DB_NAME', 'geomarketing')\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}')\n",
    "print(f'postgresql+psycopg2://{user}:***@{host}:{port}/{db}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc124a",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "This section loads the raw Excel data, cleans it by removing columns with too many missing values, imputes missing numeric values by group, removes duplicates, and saves the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e2701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "file_path = 'Data/Export_V2.xlsx'\n",
    "df_raw = pd.read_excel(file_path, sheet_name=0)\n",
    "df_raw.to_sql('raw_data', engine, index=False, if_exists='replace')\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "threshold = len(df_raw) * 0.5\n",
    "df_cleaned = df_raw.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Fill numeric columns with the median per BFS_NR group\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Grouped imputation\n",
    "df_cleaned[numeric_cols] = df_cleaned.groupby('BFS_NR')[numeric_cols].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Remove duplicates (just in case)\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "df_cleaned.to_sql('cleaned_data', engine, index=False, if_exists='replace')\n",
    "\n",
    "# Save the cleaned data\n",
    "df_cleaned.to_excel('Data/Cleaned_Data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03e8d9",
   "metadata": {},
   "source": [
    "# Load Cleaned Data to Database\n",
    "This cell loads the cleaned Excel data and uploads it to the database as a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa88f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cleaned Excel data\n",
    "df = pd.read_excel(\"Data/Cleaned_Data.xlsx\")\n",
    "\n",
    "# Push to SQL\n",
    "df.to_sql(\"gemeinden_cleaned\", engine, index=False, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8cb79",
   "metadata": {},
   "source": [
    "# Import and Upload Shapefile\n",
    "This cell loads the municipality boundaries shapefile, converts it to the correct coordinate system, and uploads it to the PostGIS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77262fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geoalchemy2 import Geometry\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "# Load shapefile\n",
    "gdf = gpd.read_file(\"Gemeindegrenzen/UP_GEMEINDEN_F.shp\")\n",
    "\n",
    "# Convert to WGS84 for Leaflet\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Connect to DB\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS gemeinden CASCADE\"))\n",
    "    conn.commit()\n",
    "\n",
    "# Upload to PostGIS with correct WGS84 SRID\n",
    "gdf.to_postgis(\n",
    "    \"gemeinden\",\n",
    "    engine,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype={\"geometry\": Geometry(\"MULTIPOLYGON\", srid=4326)}  # ‚úÖ Fix here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831f7a4",
   "metadata": {},
   "source": [
    "# Create Materialized View for Joined Data\n",
    "This cell creates a materialized view in the database by joining the geometry and cleaned attribute tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "# Define and run SQL for creating a materialized view with the join\n",
    "create_view_sql = \"\"\"\n",
    "DROP MATERIALIZED VIEW IF EXISTS gemeinden_merged;\n",
    "CREATE MATERIALIZED VIEW gemeinden_merged AS\n",
    "SELECT \n",
    "    g.*,\n",
    "    c.*\n",
    "FROM \n",
    "    gemeinden g\n",
    "JOIN \n",
    "    gemeinden_cleaned c\n",
    "ON \n",
    "    g.\"BFS\" = c.\"BFS_NR\";\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_view_sql))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"‚úÖ Materialized view 'gemeinden_merged' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e49fa",
   "metadata": {},
   "source": [
    "# Export Local Queries as GEOJSON\n",
    "This section defines a function to export selected KPIs for a given year as GeoJSON files for use in the frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# DB connection\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "\n",
    "def export_geojson(kpi_column: str, year: int):\n",
    "    filename = f\"geo_kpi_{kpi_column}_{year}.geojson\"\n",
    "    output_dir = \"Frontend/geomarketing-map/public/data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    path = os.path.join(output_dir, filename)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \"BFS\", \"GEBIET_NAME\", \"{kpi_column}\", \"Year\", geometry\n",
    "    FROM gemeinden_merged\n",
    "    WHERE \"Year\" = {year}\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_postgis(query, engine, geom_col=\"geometry\")\n",
    "    gdf = gdf.to_crs(epsg=4326)  # Ensure WGS84 for Leaflet\n",
    "\n",
    "    gdf.to_file(path, driver=\"GeoJSON\")\n",
    "    print(f\"‚úÖ Exported {kpi_column} for {year} ‚Üí {path}\")\n",
    "\n",
    "# Example usage\n",
    "export_geojson(\"Gesundheitsaufwand pro Einwohner\", 2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af44832",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation (Moran's I) for 2020\n",
    "This cell calculates and visualizes the spatial autocorrelation (Moran's I) for all KPI columns for the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541888fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "\n",
    "# Load your data for a given year\n",
    "gdf = gpd.read_postgis(\n",
    "    'SELECT * FROM gemeinden_merged WHERE \"Year\" = 2020',\n",
    "    engine,\n",
    "    geom_col=\"geometry\"\n",
    ")\n",
    "\n",
    "# Create spatial weights once\n",
    "w = Queen.from_dataframe(gdf)\n",
    "w.transform = 'r'\n",
    "\n",
    "# Non-KPI columns to exclude\n",
    "non_kpi_cols = {\n",
    "    \"BFS\", \"BFS_NR\", \"GEBIET_NAME\", \"Year\", \"geometry\",\n",
    "    \"BEZIRKSNAM\", \"ART_TEXT\", \"ART_CODE\", \"GEMEINDENA\",\n",
    "    \"ARPS\", \"SHAPE_AREA\", \"SHAPE_LEN\", \"AREA_ROUND\"\n",
    "}\n",
    "\n",
    "# Detect KPI columns\n",
    "kpi_columns = [col for col in gdf.columns if col not in non_kpi_cols and gdf[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for kpi in kpi_columns:\n",
    "    cleaned = gdf.dropna(subset=[kpi])\n",
    "    if cleaned.empty: continue\n",
    "    try:\n",
    "        mi = Moran(cleaned[kpi].values, Queen.from_dataframe(cleaned))\n",
    "        results.append({\n",
    "            \"KPI\": kpi,\n",
    "            \"Moran_I\": mi.I,\n",
    "            \"p_value\": mi.p_sim\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error for {kpi}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "moran_df = pd.DataFrame(results).sort_values(\"Moran_I\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(moran_df[\"KPI\"], moran_df[\"Moran_I\"], color=[\"#2ca02c\" if p < 0.05 else \"#cccccc\" for p in moran_df[\"p_value\"]])\n",
    "plt.axvline(0, color=\"black\", linewidth=0.8)\n",
    "plt.xlabel(\"Moran's I (2020)\")\n",
    "plt.title(\"Spatial Autocorrelation by KPI (Moran's I)\")\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb44ad",
   "metadata": {},
   "source": [
    "# Moran's I Analysis Across All Years\n",
    "This cell loops through all years, calculates Moran's I for each KPI, and saves the results for further analysis or frontend use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "\n",
    "years = range(1990, 2023)\n",
    "results = []\n",
    "\n",
    "# Columns to ignore (non-KPIs)\n",
    "non_kpi_cols = {\n",
    "    \"BFS\", \"BFS_NR\", \"GEBIET_NAME\", \"Year\", \"geometry\",\n",
    "    \"BEZIRKSNAM\", \"ART_TEXT\", \"ART_CODE\", \"GEMEINDENA\",\n",
    "    \"ARPS\", \"SHAPE_AREA\", \"SHAPE_LEN\", \"AREA_ROUND\"\n",
    "}\n",
    "\n",
    "for year in years:\n",
    "    print(f\"üìÖ Processing year: {year}\")\n",
    "    try:\n",
    "        gdf = gpd.read_postgis(\n",
    "            f'SELECT * FROM gemeinden_merged WHERE \"Year\" = {year}',\n",
    "            engine,\n",
    "            geom_col=\"geometry\"\n",
    "        )\n",
    "\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "\n",
    "        w = Queen.from_dataframe(gdf)\n",
    "        w.transform = 'r'\n",
    "\n",
    "        kpi_columns = [col for col in gdf.columns if col not in non_kpi_cols and gdf[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "        for kpi in kpi_columns:\n",
    "            cleaned = gdf.dropna(subset=[kpi])\n",
    "            if cleaned.empty:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mi = Moran(cleaned[kpi].values, Queen.from_dataframe(cleaned))\n",
    "                results.append({\n",
    "                    \"Year\": year,\n",
    "                    \"KPI\": kpi,\n",
    "                    \"Moran_I\": round(mi.I, 4),\n",
    "                    \"p_value\": round(mi.p_sim, 4)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in {year} for {kpi}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load year {year}: {e}\")\n",
    "\n",
    "# Save to CSV or JSON for frontend\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(\"Frontend/geomarketing-map/public/data/moran_results.csv\", index=False)\n",
    "df_result.to_json(\"Frontend/geomarketing-map/public/data/moran_results.json\", orient=\"records\")\n",
    "\n",
    "print(\"‚úÖ Finished Moran's I analysis across all years.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33a25d",
   "metadata": {},
   "source": [
    "# Export Static Geometry as GeoJSON\n",
    "This cell exports the static geometry of municipalities as a GeoJSON file for use in the frontend mapping application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Path to your shapefile\n",
    "shp_path = \"Gemeindegrenzen/UP_GEMEINDEN_F.shp\"  # adjust as needed\n",
    "\n",
    "# Load shapefile\n",
    "gdf = gpd.read_file(shp_path)\n",
    "\n",
    "# Simplify: keep only essential columns\n",
    "gdf = gdf[[\"BFS\", \"GEBIET_NAME\", \"geometry\"]]  # adjust if your column names differ\n",
    "\n",
    "# Convert to WGS84 for frontend mapping (Leaflet uses EPSG:4326)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Export to GeoJSON\n",
    "output_path = \"public/data/gemeinden_geometry.geojson\"\n",
    "gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"‚úÖ Exported static geometry to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02e753",
   "metadata": {},
   "source": [
    "# Database Dump and Restore to Neon\n",
    "This cell provides a script to dump the local database and restore it to a remote Neon database using environment variables for credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# === CONFIG ===\n",
    "LOCAL_DB_NAME = \"geomarketing\"\n",
    "LOCAL_DB_USER = \"postgres\"\n",
    "LOCAL_DB_PASSWORD = os.getenv(\"LOCAL_DB_PASSWORD\")\n",
    "NEON_CONNECTION = os.getenv(\"NEON_CONNECTION_STRING\")  # Full URL including password\n",
    "\n",
    "if not LOCAL_DB_PASSWORD:\n",
    "    print(\"‚ùå LOCAL_DB_PASSWORD not set in .env file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not NEON_CONNECTION:\n",
    "    print(\"‚ùå NEON_CONNECTION_STRING not set in .env file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "DUMP_FILE = f\"geomarketing_{datetime.date.today()}.dump\"\n",
    "\n",
    "# === STEP 1: Create Dump from Local ===\n",
    "def create_local_dump():\n",
    "    print(f\"üì¶ Creating dump: {DUMP_FILE}\")\n",
    "    env = os.environ.copy()\n",
    "    env[\"PGPASSWORD\"] = LOCAL_DB_PASSWORD\n",
    "    subprocess.run([\n",
    "        \"pg_dump\",\n",
    "        \"-U\", LOCAL_DB_USER,\n",
    "        \"-Fc\",  # Custom format\n",
    "        \"-f\", DUMP_FILE,\n",
    "        LOCAL_DB_NAME,\n",
    "    ], check=True, env=env)\n",
    "    print(\"‚úÖ Dump created\")\n",
    "\n",
    "# === STEP 2: Restore to Neon ===\n",
    "def restore_to_neon():\n",
    "    print(\"üîÅ Restoring to Neon...\")\n",
    "    subprocess.run([\n",
    "        \"pg_restore\",\n",
    "        \"-v\",\n",
    "        \"-d\", NEON_CONNECTION,\n",
    "        DUMP_FILE\n",
    "    ], check=True)\n",
    "    print(\"‚úÖ Restore complete\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    create_local_dump()\n",
    "    restore_to_neon()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
